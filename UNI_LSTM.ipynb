{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Long Short-Term Memory networks\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In the field of plasma physics, understanding and predicting transitions in tokamak plasmas is crucial for advancing fusion energy research. One particular area of interest is predicting back-transitions, which can significantly affect plasma stability and the overall efficiency of a tokamak.\n",
        "\n",
        "This notebook focuses on using Long Short-Term Memory (LSTM) networks, a type of recurrent neural network (RNN), to predict back-transitions in tokamak plasmas. LSTMs are well-suited for this task due to their ability to capture long-term dependencies in sequential data, making them a powerful tool for time-series prediction problems in plasma physics.\n",
        "\n",
        "The objectives of this notebook are as follows:\n",
        "\n",
        "1. Feature Selection: Test different preprocessing methods, such as Fast Fourier Transform (FFT), and select features that enhance the model’s predictive capability.\n",
        "2. Hyperparameter Optimization: Identify the best hyperparameters for the LSTM model, including the number of hidden units, layers, learning rate, and batch size, to obtain optimal performance.\n",
        "3. Performance Evaluation: Compare the model’s performance across various configurations using  metrics such as accuracy, precision, recall, and F1 score.\n",
        "4. Model Saving and Reusability: Save each trained model as a .pth file for future use and provide a dedicated cell for loading and utilizing the saved model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install \"dask[dataframe]\"\n",
        "#!pip install optuna\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import random\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Splitting: Ensuring Shot-Level Separation\n",
        "\n",
        "To ensure that time windows from a single plasma shot are not distributed across the training, validation or test sets, this notebook includes a function for data splitting. This function ensures that an entire shot is assigned exclusively to one of the subsets (train, validation or test). This approach prevents data leakage and ensures that the model generalizes well to unseen shots as it avoids overlap between subsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KqaaS8TSIIul"
      },
      "outputs": [],
      "source": [
        "def split_shots(selected_shots, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
        "    if seed is not None:\n",
        "        random.seed(seed)  # For reproducibility\n",
        "    random.shuffle(selected_shots)\n",
        "\n",
        "    total_shots = len(selected_shots)\n",
        "    train_size = int(total_shots * train_ratio)\n",
        "    val_size   = int(total_shots * val_ratio)\n",
        "\n",
        "    train_shots = selected_shots[:train_size]\n",
        "    val_shots   = selected_shots[train_size:train_size + val_size]\n",
        "    test_shots  = selected_shots[train_size + val_size:]\n",
        "    return train_shots, val_shots, test_shots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Window Function for Time-Based Prediction\n",
        "\n",
        "For predicting back-transitions, this notebook uses a window-based approach. The model is designed to make predictions within a specific time window (for example, determining if a transition will occur within the next 100 milliseconds). This approach allows the model to focus on short-term temporal dependencies while leveraging the sequential structure of the data. \n",
        "\n",
        "By framing the prediction problem in terms of time windows, we can better capture the dynamic nature of tokamak plasma measurements and provide better insights for real-time plasma control and stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b_bewuBZIIum"
      },
      "outputs": [],
      "source": [
        "def create_windows_and_labels(selected_shots_number, pq_all, HL_times,\n",
        "                              shot_number_train_set, shot_number_test_set,\n",
        "                              shot_number_val_set, window_size=0.1,\n",
        "                              global_mean=None, global_std=None):\n",
        "    \"\"\"\n",
        "    Creates the windowed data (arrays) and labels for train/val/test sets.\n",
        "    Returns: windows_train, labels_train, windows_test, labels_test, windows_val, labels_val\n",
        "    \"\"\"\n",
        "\n",
        "    if global_mean is None or global_std is None:\n",
        "        raise ValueError(\"Global mean and standard deviation must be provided.\")\n",
        "\n",
        "    windows_train, labels_train = [], []\n",
        "    windows_test,  labels_test  = [], []\n",
        "    windows_val,   labels_val   = [], []\n",
        "\n",
        "    feature_columns = None\n",
        "\n",
        "    for shot_number in selected_shots_number:\n",
        "        # Load the parquet for a single shot\n",
        "        shot_data = pd.read_parquet(pq_all[shot_number])\n",
        "        shot_data.interpolate(method='linear', inplace=True)\n",
        "        shot_data.dropna(inplace=True)\n",
        "\n",
        "        if feature_columns is None:\n",
        "            feature_columns = [col for col in shot_data.columns if col != 'time']\n",
        "\n",
        "        time_values    = shot_data['time'].values\n",
        "        feature_values = shot_data[feature_columns].values\n",
        "        transition_times = np.array(HL_times.get(shot_number, []))\n",
        "\n",
        "        start_time = time_values[0]\n",
        "        end_time   = time_values[-1]\n",
        "\n",
        "        # Determine the start and end indices for each window\n",
        "        start_indices = np.searchsorted(time_values, np.arange(start_time, end_time, window_size))\n",
        "        end_indices   = np.searchsorted(time_values, np.arange(start_time + window_size, end_time + window_size, window_size))\n",
        "\n",
        "        for start_idx, end_idx in zip(start_indices, end_indices):\n",
        "            if start_idx == end_idx:  # skip empty windows\n",
        "                continue\n",
        "\n",
        "            # Standardize using global stats\n",
        "            window_array = (feature_values[start_idx:end_idx] - global_mean.values) / global_std.values\n",
        "\n",
        "            # Label the window if it contains an HL transition time\n",
        "            current_time_start = time_values[start_idx]\n",
        "            current_time_end   = time_values[end_idx - 1]\n",
        "            label = int(np.any((transition_times >= current_time_start) & (transition_times < current_time_end)))\n",
        "\n",
        "            # Append to the appropriate dataset\n",
        "            if shot_number in shot_number_train_set:\n",
        "                windows_train.append(window_array)\n",
        "                labels_train.append(label)\n",
        "            elif shot_number in shot_number_test_set:\n",
        "                windows_test.append(window_array)\n",
        "                labels_test.append(label)\n",
        "            elif shot_number in shot_number_val_set:\n",
        "                windows_val.append(window_array)\n",
        "                labels_val.append(label)\n",
        "\n",
        "    # Debug prints\n",
        "    print(f\"Number of windows_train: {len(windows_train)}\")\n",
        "    if len(windows_train) > 0:\n",
        "        print(f\"Shape of one window (train example): {windows_train[0].shape}\")\n",
        "    print(f\"Number of windows_test: {len(windows_test)}\")\n",
        "    print(f\"Number of windows_val: {len(windows_val)}\")\n",
        "\n",
        "    return windows_train, labels_train, windows_test, labels_test, windows_val, labels_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i8AMCaO3IIun"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "#     PyTorch Dataset\n",
        "# -----------------------\n",
        "class HLTransitionWindowDataset(Dataset):\n",
        "    def __init__(self, windowed_data, windowed_labels):\n",
        "        self.windowed_data = windowed_data\n",
        "        self.windowed_labels = windowed_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.windowed_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = torch.FloatTensor(self.windowed_data[idx])   # shape: (window_length, num_features)\n",
        "        y = torch.FloatTensor([self.windowed_labels[idx]])\n",
        "        return X, y\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function that pads variable-length sequences in the same batch.\n",
        "    Returns padded X, labels, and the original lengths.\n",
        "    \"\"\"\n",
        "    X_batch, y_batch = zip(*batch)\n",
        "    lengths = [x.size(0) for x in X_batch]  # original lengths for each sequence\n",
        "\n",
        "    # Pad sequences to max length in the batch\n",
        "    X_batch_padded = pad_sequence(X_batch, batch_first=True)  # shape: (batch_size, max_length, num_features)\n",
        "\n",
        "    y_batch = torch.stack(y_batch).float()  # shape: (batch_size, 1)\n",
        "    return X_batch_padded, y_batch, torch.tensor(lengths)\n",
        "\n",
        "# -------------------------\n",
        "#     Model Definition\n",
        "# -------------------------\n",
        "class HLTransitionPredictor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(HLTransitionPredictor, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc   = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_length, input_size)\n",
        "        lengths: the original sequence lengths before padding.\n",
        "        \"\"\"\n",
        "        # Pack the padded input to optimize LSTM computation\n",
        "        packed_input = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_output, (h_n, _) = self.lstm(packed_input)\n",
        "        # Use the final hidden state from the last layer of LSTM\n",
        "        out = self.fc(h_n[-1])  # h_n[-1] is the hidden state for the last LSTM layer\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZvLRS-0TIIuo"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "#     Evaluation Function\n",
        "# ----------------------------\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch, lengths in data_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            outputs = torch.sigmoid(model(X_batch, lengths))\n",
        "            # Squeeze to match shapes if necessary\n",
        "            if outputs.dim() == 2 and outputs.shape[1] == 1:\n",
        "                outputs = outputs.squeeze(dim=1)\n",
        "            if y_batch.dim() == 2 and y_batch.shape[1] == 1:\n",
        "                y_batch = y_batch.squeeze(dim=1)\n",
        "\n",
        "            preds = (outputs > 0.5).float()\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "    return accuracy, precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fyPqEuuhIIup"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "#  Function to Get DataLoaders\n",
        "# -----------------------------\n",
        "def get_data(pq_all, HL_times, selected_shots_number,\n",
        "             train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42, window_size=0.1):\n",
        "    # Split shots\n",
        "    train_shots, val_shots, test_shots = split_shots(\n",
        "        selected_shots_number,\n",
        "        train_ratio=train_ratio,\n",
        "        val_ratio=val_ratio,\n",
        "        test_ratio=test_ratio,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    # Compute global mean/std using Dask\n",
        "    ddf = dd.concat([dd.read_parquet(pq_all[shot]) for shot in selected_shots_number])\n",
        "    feature_columns = [col for col in ddf.columns if col != 'time']\n",
        "    global_mean = ddf[feature_columns].mean().compute()\n",
        "    global_std  = ddf[feature_columns].std().compute()\n",
        "\n",
        "    # Create windows and labels\n",
        "    windows_train, labels_train, windows_test, labels_test, windows_val, labels_val = create_windows_and_labels(\n",
        "        selected_shots_number, pq_all, HL_times,\n",
        "        shot_number_train_set=train_shots,\n",
        "        shot_number_test_set=test_shots,\n",
        "        shot_number_val_set=val_shots,\n",
        "        window_size=window_size,\n",
        "        global_mean=global_mean,\n",
        "        global_std=global_std\n",
        "    )\n",
        "\n",
        "    # Create Datasets\n",
        "    train_dataset = HLTransitionWindowDataset(windows_train, labels_train)\n",
        "    val_dataset   = HLTransitionWindowDataset(windows_val,   labels_val)\n",
        "    test_dataset  = HLTransitionWindowDataset(windows_test,  labels_test)\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset, feature_columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vpr_n9xZIIup"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "#    Training for One Epoch\n",
        "# -----------------------------\n",
        "# The purpose of training for a single epoch is to update the model’s parameters \n",
        "# once using the entire training dataset. \n",
        "# This allows us to measure how well the model is learning and adjust hyperparameters \n",
        "# or stop early if necessary.\n",
        "\n",
        "def train_one_epoch(model, criterion, optimizer, data_loader, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for X_batch, y_batch, lengths in data_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        lengths = lengths.to(device)\n",
        "\n",
        "        outputs = model(X_batch, lengths)  # shape: (batch_size, 1)\n",
        "\n",
        "        # Ensure y_batch has shape (batch_size, 1)\n",
        "        if y_batch.dim() == 1:\n",
        "            y_batch = y_batch.unsqueeze(1)\n",
        "\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "    # Compute training metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "\n",
        "    return total_loss / len(data_loader), f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter Tuning with Optuna\n",
        "\n",
        "Using Optuna allows us to efficiently search for the best combination of hyperparameters. This approach leverages a trial-based optimization strategy to find hyperparameters that minimize the validation loss in order to improve the model's predictive performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rFfyiYEsIIup"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "#   Optuna Objective Function\n",
        "# -----------------------------\n",
        "def objective(trial, train_dataset, val_dataset, feature_columns, device):\n",
        "    \"\"\"\n",
        "    The objective function to be optimized by Optuna.\n",
        "    We define a small search space for hidden_size, num_layers, and learning_rate.\n",
        "    You can expand or adjust as needed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Hyperparameter search space\n",
        "    hidden_size   = trial.suggest_int(\"hidden_size\", 32, 128, step=16)\n",
        "    num_layers    = trial.suggest_int(\"num_layers\", 1, 3)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
        "    batch_size    = trial.suggest_categorical(\"batch_size\", [4, 8, 16, 32])\n",
        "\n",
        "    # Print which trial is being started + chosen params\n",
        "    print(f\"\\n--- Trial {trial.number} / Params: hidden_size={hidden_size}, \"\n",
        "          f\"num_layers={num_layers}, learning_rate={learning_rate:.5f}, batch_size={batch_size} ---\")\n",
        "\n",
        "    # Create data loaders with the trial-specific batch size\n",
        "    train_data_loader = DataLoader(train_dataset,\n",
        "                                   batch_size=batch_size,\n",
        "                                   shuffle=True,\n",
        "                                   collate_fn=collate_fn)\n",
        "    val_data_loader   = DataLoader(val_dataset,\n",
        "                                   batch_size=batch_size,\n",
        "                                   shuffle=False,\n",
        "                                   collate_fn=collate_fn)\n",
        "\n",
        "    # Count classes in train dataset to build WeightedRandomSampler or pos_weight if desired\n",
        "    labels_train = [lbl for lbl in train_dataset.windowed_labels]\n",
        "    label_counts = Counter(labels_train)\n",
        "    num_negative = label_counts.get(0, 0)\n",
        "    num_positive = label_counts.get(1, 0)\n",
        "\n",
        "    if num_positive == 0:\n",
        "        pos_weight = torch.tensor([1.0], dtype=torch.float32).to(device)\n",
        "    else:\n",
        "        pos_weight = torch.tensor([num_negative / max(1, num_positive)], dtype=torch.float32).to(device)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    # Build model, optimizer\n",
        "    input_size = len(feature_columns)\n",
        "    model = HLTransitionPredictor(input_size, hidden_size, num_layers).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    num_epochs = 10\n",
        "    best_val_f1 = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"[Trial {trial.number}] Epoch {epoch+1}/{num_epochs} ...\")\n",
        "        train_loss, train_f1 = train_one_epoch(model, criterion, optimizer, train_data_loader, device)\n",
        "        val_accuracy, val_precision, val_recall, val_f1 = evaluate_model(model, val_data_loader, device)\n",
        "\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f}\")\n",
        "\n",
        "        # Report intermediate results to Optuna\n",
        "        trial.report(val_f1, epoch)\n",
        "\n",
        "        # Check for pruning\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "\n",
        "    # Return the best validation F1 of this trial\n",
        "    return best_val_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mjp-RuR9IIuq"
      },
      "outputs": [],
      "source": [
        "def run_optuna_optimization(pq_path, hl_json_path, selected_shots_number, n_trials=20, seed=42):\n",
        "    \"\"\"\n",
        "    Master function that:\n",
        "    1) Loads the data,\n",
        "    2) Creates train/val/test sets,\n",
        "    3) Runs Optuna study to find best hyperparams,\n",
        "    4) Retrains and evaluates the final model on test set,\n",
        "    5) Saves the final model weights.\n",
        "    \"\"\"\n",
        "    pq_all_files = glob.glob(os.path.join(pq_path, '*_processed.parquet'))\n",
        "    pq_all = {int(os.path.basename(f).split(\"_processed.parquet\")[0]): f for f in pq_all_files}\n",
        "\n",
        "    with open(hl_json_path, 'r') as f:\n",
        "        HL_times = json.load(f)\n",
        "        HL_times = {int(k): v for k, v in HL_times.items()}\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset, feature_columns = get_data(\n",
        "        pq_all=pq_all,\n",
        "        HL_times=HL_times,\n",
        "        selected_shots_number=selected_shots_number,\n",
        "        train_ratio=0.7,\n",
        "        val_ratio=0.15,\n",
        "        test_ratio=0.15,\n",
        "        seed=seed,\n",
        "        window_size=0.1\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def optuna_objective(trial):\n",
        "        return objective(trial, train_dataset, val_dataset, feature_columns, device)\n",
        "\n",
        "    study = optuna.create_study(\n",
        "        direction=\"maximize\",\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=2, interval_steps=3)\n",
        "    )\n",
        "    study.optimize(optuna_objective, n_trials=n_trials)\n",
        "\n",
        "    print(\"\\n--- Optuna Study Results ---\")\n",
        "    print(\"Best Trial F1 Score:\", study.best_value)\n",
        "    print(\"Best Hyperparameters:\", study.best_params)\n",
        "\n",
        "    best_params = study.best_params\n",
        "    best_hidden_size   = best_params[\"hidden_size\"]\n",
        "    best_num_layers    = best_params[\"num_layers\"]\n",
        "    best_learning_rate = best_params[\"learning_rate\"]\n",
        "    best_batch_size    = best_params[\"batch_size\"]\n",
        "\n",
        "    input_size  = len(feature_columns)\n",
        "    final_model = HLTransitionPredictor(input_size, best_hidden_size, best_num_layers).to(device)\n",
        "\n",
        "    labels_train = [lbl for lbl in train_dataset.windowed_labels]\n",
        "    label_counts = Counter(labels_train)\n",
        "    num_negative = label_counts.get(0, 0)\n",
        "    num_positive = label_counts.get(1, 0)\n",
        "    if num_positive == 0:\n",
        "        pos_weight = torch.tensor([1.0], dtype=torch.float32).to(device)\n",
        "    else:\n",
        "        pos_weight = torch.tensor([num_negative / max(1, num_positive)], dtype=torch.float32).to(device)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    optimizer = torch.optim.Adam(final_model.parameters(), lr=best_learning_rate)\n",
        "\n",
        "    train_data_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    val_data_loader   = DataLoader(val_dataset,   batch_size=best_batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "    test_data_loader  = DataLoader(test_dataset,  batch_size=best_batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # Retrain final model\n",
        "    num_epochs = 20\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_f1 = train_one_epoch(final_model, criterion, optimizer, train_data_loader, device)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_accuracy, test_precision, test_recall, test_f1 = evaluate_model(final_model, test_data_loader, device)\n",
        "    print(\"\\n--- Final Test Results with Best Hyperparams ---\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Test Precision: {test_precision:.4f}\")\n",
        "    print(f\"Test Recall: {test_recall:.4f}\")\n",
        "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "\n",
        "    # 8) SAVE the final model checkpoint\n",
        "    torch.save(final_model.state_dict(), \"/content/drive/MyDrive/LSTM_0_final.pth\")\n",
        "    print(\"Final model weights saved to /content/drive/MyDrive/LSTM_0_final.pth\")\n",
        "\n",
        "    return study\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wrapping Up for Optimization\n",
        "\n",
        "In this section, we bring everything together to perform hyperparameter optimization using Optuna. The shot numbers are extracted from the filenames to define the specific data to be used during optimization. \n",
        "\n",
        "Finally, we call the `run_optuna_optimization` function, which does the process of searching for the best hyperparameters. This step enables us to fine-tune the model for optimal performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEfPyrvIIIur",
        "outputId": "de033438-d596-4ec6-ca0a-5598a13257e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-12-18 00:14:41,204] A new study created in memory with name: no-name-12946eaf-d5c3-4462-815b-9f8598a0cd3a\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of windows_train: 2613\n",
            "Shape of one window (train example): (1000, 143)\n",
            "Number of windows_test: 545\n",
            "Number of windows_val: 561\n",
            "\n",
            "--- Trial 0 / Params: hidden_size=80, num_layers=3, learning_rate=0.00112, batch_size=32 ---\n",
            "[Trial 0] Epoch 1/10 ...\n",
            "  Train Loss: 0.9664 | Train F1: 0.3857 | Val F1: 0.4074\n",
            "[Trial 0] Epoch 2/10 ...\n",
            "  Train Loss: 0.8355 | Train F1: 0.4607 | Val F1: 0.4843\n",
            "[Trial 0] Epoch 3/10 ...\n",
            "  Train Loss: 0.7339 | Train F1: 0.4972 | Val F1: 0.4952\n",
            "[Trial 0] Epoch 4/10 ...\n",
            "  Train Loss: 0.8081 | Train F1: 0.4923 | Val F1: 0.4860\n",
            "[Trial 0] Epoch 5/10 ...\n",
            "  Train Loss: 0.7163 | Train F1: 0.5554 | Val F1: 0.4762\n",
            "[Trial 0] Epoch 6/10 ...\n",
            "  Train Loss: 0.6498 | Train F1: 0.5682 | Val F1: 0.4746\n",
            "[Trial 0] Epoch 7/10 ...\n",
            "  Train Loss: 0.6588 | Train F1: 0.5685 | Val F1: 0.5032\n",
            "[Trial 0] Epoch 8/10 ...\n",
            "  Train Loss: 0.6267 | Train F1: 0.5839 | Val F1: 0.4935\n",
            "[Trial 0] Epoch 9/10 ...\n",
            "  Train Loss: 0.6463 | Train F1: 0.5723 | Val F1: 0.4118\n",
            "[Trial 0] Epoch 10/10 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-12-18 00:16:15,455] Trial 0 finished with value: 0.5032258064516129 and parameters: {'hidden_size': 80, 'num_layers': 3, 'learning_rate': 0.0011223600526648913, 'batch_size': 32}. Best is trial 0 with value: 0.5032258064516129.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train Loss: 0.6244 | Train F1: 0.5877 | Val F1: 0.4425\n",
            "\n",
            "--- Trial 1 / Params: hidden_size=80, num_layers=2, learning_rate=0.00405, batch_size=4 ---\n",
            "[Trial 1] Epoch 1/10 ...\n",
            "  Train Loss: 1.0115 | Train F1: 0.3945 | Val F1: 0.3723\n",
            "[Trial 1] Epoch 2/10 ...\n",
            "  Train Loss: 0.8934 | Train F1: 0.4169 | Val F1: 0.4706\n",
            "[Trial 1] Epoch 3/10 ...\n",
            "  Train Loss: 0.7792 | Train F1: 0.4861 | Val F1: 0.5000\n",
            "[Trial 1] Epoch 4/10 ...\n",
            "  Train Loss: 0.7226 | Train F1: 0.5217 | Val F1: 0.4830\n",
            "[Trial 1] Epoch 5/10 ...\n",
            "  Train Loss: 0.7033 | Train F1: 0.5144 | Val F1: 0.4664\n",
            "[Trial 1] Epoch 6/10 ...\n",
            "  Train Loss: 0.6800 | Train F1: 0.5346 | Val F1: 0.5481\n",
            "[Trial 1] Epoch 7/10 ...\n",
            "  Train Loss: 0.6304 | Train F1: 0.5680 | Val F1: 0.5545\n",
            "[Trial 1] Epoch 8/10 ...\n",
            "  Train Loss: 0.6234 | Train F1: 0.5415 | Val F1: 0.5040\n",
            "[Trial 1] Epoch 9/10 ...\n",
            "  Train Loss: 0.6752 | Train F1: 0.5414 | Val F1: 0.5200\n",
            "[Trial 1] Epoch 10/10 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-12-18 00:22:39,003] Trial 1 finished with value: 0.5545454545454546 and parameters: {'hidden_size': 80, 'num_layers': 2, 'learning_rate': 0.004045968458676973, 'batch_size': 4}. Best is trial 1 with value: 0.5545454545454546.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train Loss: 0.6536 | Train F1: 0.5276 | Val F1: 0.5198\n",
            "\n",
            "--- Trial 2 / Params: hidden_size=128, num_layers=2, learning_rate=0.00046, batch_size=8 ---\n",
            "[Trial 2] Epoch 1/10 ...\n",
            "  Train Loss: 0.8898 | Train F1: 0.4158 | Val F1: 0.4356\n",
            "[Trial 2] Epoch 2/10 ...\n",
            "  Train Loss: 0.7836 | Train F1: 0.4905 | Val F1: 0.4679\n",
            "[Trial 2] Epoch 3/10 ...\n",
            "  Train Loss: 0.7517 | Train F1: 0.5114 | Val F1: 0.5100\n",
            "[Trial 2] Epoch 4/10 ...\n",
            "  Train Loss: 0.7070 | Train F1: 0.5365 | Val F1: 0.4286\n",
            "[Trial 2] Epoch 5/10 ...\n",
            "  Train Loss: 0.7158 | Train F1: 0.5032 | Val F1: 0.4480\n",
            "[Trial 2] Epoch 6/10 ...\n",
            "  Train Loss: 0.6704 | Train F1: 0.5426 | Val F1: 0.4932\n",
            "[Trial 2] Epoch 7/10 ...\n",
            "  Train Loss: 0.6290 | Train F1: 0.5732 | Val F1: 0.5143\n",
            "[Trial 2] Epoch 8/10 ...\n",
            "  Train Loss: 0.6107 | Train F1: 0.5746 | Val F1: 0.4933\n",
            "[Trial 2] Epoch 9/10 ...\n",
            "  Train Loss: 0.6077 | Train F1: 0.5979 | Val F1: 0.5213\n",
            "[Trial 2] Epoch 10/10 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-12-18 00:26:06,048] Trial 2 finished with value: 0.5326633165829145 and parameters: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0004632611560391112, 'batch_size': 8}. Best is trial 1 with value: 0.5545454545454546.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train Loss: 0.6537 | Train F1: 0.5392 | Val F1: 0.5327\n",
            "\n",
            "--- Trial 3 / Params: hidden_size=112, num_layers=3, learning_rate=0.00210, batch_size=16 ---\n",
            "[Trial 3] Epoch 1/10 ...\n",
            "  Train Loss: 1.0094 | Train F1: 0.4037 | Val F1: 0.4175\n",
            "[Trial 3] Epoch 2/10 ...\n",
            "  Train Loss: 0.9185 | Train F1: 0.4269 | Val F1: 0.4607\n",
            "[Trial 3] Epoch 3/10 ...\n",
            "  Train Loss: 0.8963 | Train F1: 0.4367 | Val F1: 0.4408\n",
            "[Trial 3] Epoch 4/10 ...\n",
            "  Train Loss: 0.8659 | Train F1: 0.4860 | Val F1: 0.4475\n",
            "[Trial 3] Epoch 5/10 ...\n",
            "  Train Loss: 0.8582 | Train F1: 0.4928 | Val F1: 0.4583\n",
            "[Trial 3] Epoch 6/10 ...\n",
            "  Train Loss: 0.8103 | Train F1: 0.5184 | Val F1: 0.4923\n",
            "[Trial 3] Epoch 7/10 ...\n",
            "  Train Loss: 0.7353 | Train F1: 0.5533 | Val F1: 0.4854\n",
            "[Trial 3] Epoch 8/10 ...\n",
            "  Train Loss: 0.7178 | Train F1: 0.5338 | Val F1: 0.4750\n",
            "[Trial 3] Epoch 9/10 ...\n",
            "  Train Loss: 0.7408 | Train F1: 0.4680 | Val F1: 0.4453\n",
            "[Trial 3] Epoch 10/10 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-12-18 00:28:49,606] Trial 3 finished with value: 0.49572649572649574 and parameters: {'hidden_size': 112, 'num_layers': 3, 'learning_rate': 0.002104517917842021, 'batch_size': 16}. Best is trial 1 with value: 0.5545454545454546.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train Loss: 0.6899 | Train F1: 0.5392 | Val F1: 0.4957\n",
            "\n",
            "--- Trial 4 / Params: hidden_size=64, num_layers=1, learning_rate=0.00862, batch_size=8 ---\n",
            "[Trial 4] Epoch 1/10 ...\n",
            "  Train Loss: 0.9127 | Train F1: 0.4171 | Val F1: 0.3885\n",
            "[Trial 4] Epoch 2/10 ...\n",
            "  Train Loss: 0.8164 | Train F1: 0.4450 | Val F1: 0.4191\n",
            "[Trial 4] Epoch 3/10 ...\n",
            "  Train Loss: 0.7040 | Train F1: 0.5058 | Val F1: 0.4167\n",
            "[Trial 4] Epoch 4/10 ...\n",
            "  Train Loss: 0.6934 | Train F1: 0.4926 | Val F1: 0.4851\n",
            "[Trial 4] Epoch 5/10 ...\n",
            "  Train Loss: 0.6238 | Train F1: 0.5257 | Val F1: 0.5259\n",
            "[Trial 4] Epoch 6/10 ...\n",
            "  Train Loss: 0.5647 | Train F1: 0.5676 | Val F1: 0.5687\n",
            "[Trial 4] Epoch 7/10 ...\n",
            "  Train Loss: 0.6008 | Train F1: 0.5775 | Val F1: 0.5401\n",
            "[Trial 4] Epoch 8/10 ...\n",
            "  Train Loss: 0.6064 | Train F1: 0.5898 | Val F1: 0.4936\n",
            "[Trial 4] Epoch 9/10 ...\n",
            "  Train Loss: 0.6025 | Train F1: 0.5588 | Val F1: 0.5520\n",
            "[Trial 4] Epoch 10/10 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-12-18 00:30:32,916] Trial 4 finished with value: 0.6113989637305699 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.008621736129726262, 'batch_size': 8}. Best is trial 4 with value: 0.6113989637305699.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train Loss: 0.5378 | Train F1: 0.5811 | Val F1: 0.6114\n",
            "\n",
            "--- Trial 5 / Params: hidden_size=112, num_layers=3, learning_rate=0.00089, batch_size=32 ---\n",
            "[Trial 5] Epoch 1/10 ...\n",
            "  Train Loss: 0.9358 | Train F1: 0.3831 | Val F1: 0.4732\n",
            "[Trial 5] Epoch 2/10 ...\n",
            "  Train Loss: 0.7811 | Train F1: 0.4672 | Val F1: 0.4418\n",
            "[Trial 5] Epoch 3/10 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-12-18 00:31:03,630] Trial 5 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train Loss: 0.7504 | Train F1: 0.5009 | Val F1: 0.4327\n",
            "\n",
            "--- Trial 6 / Params: hidden_size=112, num_layers=2, learning_rate=0.00062, batch_size=4 ---\n",
            "[Trial 6] Epoch 1/10 ...\n",
            "  Train Loss: 0.9378 | Train F1: 0.4125 | Val F1: 0.3878\n",
            "[Trial 6] Epoch 2/10 ...\n",
            "  Train Loss: 0.8038 | Train F1: 0.4637 | Val F1: 0.4647\n",
            "[Trial 6] Epoch 3/10 ...\n",
            "  Train Loss: 0.8021 | Train F1: 0.4797 | Val F1: 0.5161\n",
            "[Trial 6] Epoch 4/10 ...\n",
            "  Train Loss: 0.7507 | Train F1: 0.5352 | Val F1: 0.4716\n",
            "[Trial 6] Epoch 5/10 ...\n",
            "  Train Loss: 0.7436 | Train F1: 0.5118 | Val F1: 0.4622\n",
            "[Trial 6] Epoch 6/10 ...\n",
            "  Train Loss: 0.7259 | Train F1: 0.5262 | Val F1: 0.4146\n",
            "[Trial 6] Epoch 7/10 ...\n",
            "  Train Loss: 0.7003 | Train F1: 0.5290 | Val F1: 0.4291\n",
            "[Trial 6] Epoch 8/10 ...\n",
            "  Train Loss: 0.6630 | Train F1: 0.5374 | Val F1: 0.4685\n",
            "[Trial 6] Epoch 9/10 ...\n",
            "  Train Loss: 0.6364 | Train F1: 0.5519 | Val F1: 0.5285\n",
            "[Trial 6] Epoch 10/10 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-12-18 00:37:38,901] Trial 6 finished with value: 0.5284974093264249 and parameters: {'hidden_size': 112, 'num_layers': 2, 'learning_rate': 0.0006171017963585639, 'batch_size': 4}. Best is trial 4 with value: 0.6113989637305699.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train Loss: 0.6270 | Train F1: 0.5738 | Val F1: 0.4328\n",
            "\n",
            "--- Trial 7 / Params: hidden_size=64, num_layers=1, learning_rate=0.00144, batch_size=32 ---\n",
            "[Trial 7] Epoch 1/10 ...\n",
            "  Train Loss: 0.9155 | Train F1: 0.4006 | Val F1: 0.4426\n",
            "[Trial 7] Epoch 2/10 ...\n",
            "  Train Loss: 0.7952 | Train F1: 0.4575 | Val F1: 0.4427\n",
            "[Trial 7] Epoch 3/10 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-12-18 00:37:52,803] Trial 7 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train Loss: 0.7326 | Train F1: 0.4940 | Val F1: 0.4661\n",
            "\n",
            "--- Trial 8 / Params: hidden_size=96, num_layers=1, learning_rate=0.00013, batch_size=8 ---\n",
            "[Trial 8] Epoch 1/10 ...\n",
            "  Train Loss: 1.0066 | Train F1: 0.3577 | Val F1: 0.3918\n",
            "[Trial 8] Epoch 2/10 ...\n",
            "  Train Loss: 0.8675 | Train F1: 0.4241 | Val F1: 0.4417\n",
            "[Trial 8] Epoch 3/10 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-12-18 00:38:29,406] Trial 8 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train Loss: 0.7736 | Train F1: 0.4829 | Val F1: 0.4630\n",
            "\n",
            "--- Trial 9 / Params: hidden_size=64, num_layers=2, learning_rate=0.00210, batch_size=8 ---\n",
            "[Trial 9] Epoch 1/10 ...\n",
            "  Train Loss: 1.0408 | Train F1: 0.3519 | Val F1: 0.3647\n",
            "[Trial 9] Epoch 2/10 ...\n",
            "  Train Loss: 0.8653 | Train F1: 0.4112 | Val F1: 0.4272\n",
            "[Trial 9] Epoch 3/10 ...\n",
            "  Train Loss: 0.7838 | Train F1: 0.4609 | Val F1: 0.4979\n",
            "[Trial 9] Epoch 4/10 ...\n",
            "  Train Loss: 0.8285 | Train F1: 0.4613 | Val F1: 0.4603\n",
            "[Trial 9] Epoch 5/10 ...\n",
            "  Train Loss: 0.7860 | Train F1: 0.4839 | Val F1: 0.4496\n",
            "[Trial 9] Epoch 6/10 ...\n",
            "  Train Loss: 0.6970 | Train F1: 0.5153 | Val F1: 0.4565\n",
            "[Trial 9] Epoch 7/10 ...\n",
            "  Train Loss: 0.7202 | Train F1: 0.5099 | Val F1: 0.4739\n",
            "[Trial 9] Epoch 8/10 ...\n",
            "  Train Loss: 0.6884 | Train F1: 0.5129 | Val F1: 0.4372\n",
            "[Trial 9] Epoch 9/10 ...\n",
            "  Train Loss: 0.6747 | Train F1: 0.5385 | Val F1: 0.5374\n",
            "[Trial 9] Epoch 10/10 ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-12-18 00:41:32,976] Trial 9 finished with value: 0.5463414634146342 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.0020989443191869964, 'batch_size': 8}. Best is trial 4 with value: 0.6113989637305699.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Train Loss: 0.6152 | Train F1: 0.5573 | Val F1: 0.5463\n",
            "\n",
            "--- Optuna Study Results ---\n",
            "Best Trial F1 Score: 0.6113989637305699\n",
            "Best Hyperparameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.008621736129726262, 'batch_size': 8}\n",
            "\n",
            "--- Final Test Results with Best Hyperparams ---\n",
            "Test Accuracy: 0.8532\n",
            "Test Precision: 0.4454\n",
            "Test Recall: 0.7910\n",
            "Test F1 Score: 0.5699\n",
            "Final model weights saved to /content/drive/MyDrive/LSTM_0_final.pth\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "pq_path = \"/content/drive/MyDrive/preprocessed_features0\"\n",
        "hl_json_path = \"/content/drive/MyDrive/HL_times.json\"\n",
        "\n",
        "pq_all_files = glob.glob(os.path.join(pq_path, '*_processed.parquet'))\n",
        "\n",
        "selected_shots_number = [int(os.path.basename(f).split(\"_processed.parquet\")[0]) for f in pq_all_files]\n",
        "\n",
        "\n",
        "    # Run Optuna optimization\n",
        "study = run_optuna_optimization(\n",
        "      pq_path=pq_path,\n",
        "      hl_json_path=hl_json_path,\n",
        "      selected_shots_number=selected_shots_number,\n",
        "      n_trials=10,   # number of trials for hyperparameter search\n",
        "      seed=42\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EdxLI8BWDCo"
      },
      "outputs": [],
      "source": [
        "# Display the best F1 Score and hyperparameters associated\n",
        "print(\"Best Trial F1 Score:\", study.best_value)\n",
        "print(\"Best Hyperparameters:\", study.best_params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using the Final Model\n",
        "\n",
        "This cell has all components to utilize the trained LSTM model for evaluation and predictions. It ensures access to preprocessed data and model weights, sets up reproducibility and prepares DataLoaders with optimal settings. The final model is evaluated on the test set and its performance metrics (accuracy, precision, recall, F1 score) are reported alongside a confusion matrix for detailed analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "SacObczIXBkZ",
        "outputId": "5660434e-456d-4049-f03a-e7321d7adf60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Number of windows_train: 2613\n",
            "Shape of one window (train example): (1000, 143)\n",
            "Number of windows_test: 545\n",
            "Number of windows_val: 561\n",
            "Input size (number of features): 143\n",
            "Model weights loaded from /content/drive/MyDrive/LSTM_0_final.pth\n",
            "\n",
            "--- Final Model Performance on Test Set ---\n",
            "Accuracy :  0.8532\n",
            "Precision: 0.4454\n",
            "Recall   : 0.7910\n",
            "F1 Score : 0.5699\n",
            "Confusion matrix plot saved to /content/drive/MyDrive/cmfinal_with_preprocess0noFFT.png\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#!pip install \"dask[dataframe]\"\n",
        "#!pip install optuna\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import random\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# To ensure reproducibility \n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Best hyperparameters for this preprocessed file (id=0 no FFT)\n",
        "best_params = {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.008621736129726262, 'batch_size': 8}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1) Define the same model class used during training\n",
        "class HLTransitionPredictor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(HLTransitionPredictor, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        packed_input = nn.utils.rnn.pack_padded_sequence(\n",
        "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        packed_output, (h_n, _) = self.lstm(packed_input)\n",
        "        out = self.fc(h_n[-1])  # h_n[-1] = hidden state of the last LSTM layer\n",
        "        return out\n",
        "\n",
        "# 2) Prepare data and compute input size dynamically\n",
        "pq_path = \"/content/drive/MyDrive/preprocessed_features0\"\n",
        "hl_json_path = \"/content/drive/MyDrive/HL_times.json\"\n",
        "pq_all_files = glob.glob(os.path.join(pq_path, '*_processed.parquet'))\n",
        "pq_all = {int(os.path.basename(f).split(\"_processed.parquet\")[0]): f for f in pq_all_files}\n",
        "\n",
        "selected_shots_number = [int(os.path.basename(f).split(\"_processed.parquet\")[0]) for f in pq_all_files]\n",
        "\n",
        "with open(hl_json_path, 'r') as f:\n",
        "        HL_times = json.load(f)\n",
        "        HL_times = {int(k): v for k, v in HL_times.items()}\n",
        "\n",
        "train_dataset, val_dataset, test_dataset, feature_columns = get_data(\n",
        "    pq_all=pq_all,\n",
        "    HL_times=HL_times,\n",
        "    selected_shots_number=selected_shots_number,\n",
        "    train_ratio=0.7,\n",
        "    val_ratio=0.15,\n",
        "    test_ratio=0.15,\n",
        "    seed=SEED,  # for reproducibility\n",
        "    window_size=0.1\n",
        ")\n",
        "input_size = len(feature_columns)  # Determine input size\n",
        "print(f\"Input size (number of features): {input_size}\")\n",
        "\n",
        "# 3) Create and load the model\n",
        "final_model = HLTransitionPredictor(\n",
        "    input_size=input_size,\n",
        "    hidden_size=best_params['hidden_size'],\n",
        "    num_layers=best_params['num_layers']\n",
        ").to(device)\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/LSTM_0_final.pth\"\n",
        "final_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "final_model.eval()\n",
        "print(f\"Model weights loaded from {model_path}\")\n",
        "\n",
        "# 4) Use DataLoaders with best batch_size\n",
        "train_data_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=best_params['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "val_data_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=best_params['batch_size'],\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "test_data_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=best_params['batch_size'],\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch, lengths in data_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = torch.sigmoid(model(X_batch, lengths))\n",
        "            preds   = (outputs > 0.5).float().cpu().numpy().flatten()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(y_batch.cpu().numpy().flatten())\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "    return accuracy, precision, recall, f1, np.array(all_labels), np.array(all_preds)\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names, save_path=None):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    if save_path is not None:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"Confusion matrix plot saved to {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "# 5) Evaluate final model on test set\n",
        "test_accuracy, test_precision, test_recall, test_f1, y_true, y_pred = evaluate_model(final_model, test_data_loader, device)\n",
        "\n",
        "print(\"\\n--- Final Model Performance on Test Set ---\")\n",
        "print(f\"Accuracy :  {test_accuracy:.4f}\")\n",
        "print(f\"Precision: {test_precision:.4f}\")\n",
        "print(f\"Recall   : {test_recall:.4f}\")\n",
        "print(f\"F1 Score : {test_f1:.4f}\")\n",
        "\n",
        "# 6) Plot confusion matrix\n",
        "class_names = [\"No HL\", \"HL\"]\n",
        "plot_confusion_matrix(y_true, y_pred, class_names, save_path=\"/content/drive/MyDrive/cmfinal_with_preprocess0noFFT.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blYacqvifbvk"
      },
      "source": [
        "# Model Performance Summary\n",
        "\n",
        "| Model Configuration                              | Accuracy | Precision | Recall  | F1 Score | Best Parameters                                                                 |\n",
        "|--------------------------------------------------|----------|-----------|---------|----------|---------------------------------------------------------------------------------|\n",
        "| LSTM with no preprocessing                       | 0.8068   | 0.3143    | 0.8250  | 0.4552   | {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.001}                   |\n",
        "| LSTM with selected features 0, no FFT for Halpha | 0.8532   | 0.4454    | 0.7910  | 0.5699   | {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.0086, 'batch_size': 8} |\n",
        "| LSTM with selected features 0, FFT for Halpha    | 0.8017   | 0.3563    | 0.9254  | 0.5145   | {'hidden_size': 96, 'num_layers': 1, 'learning_rate': 0.0083, 'batch_size': 16}|\n",
        "| LSTM with selected features 1, no FFT for Halpha | 0.8399   | 0.4203    | 0.8529  | 0.5631   | {'hidden_size': 96, 'num_layers': 2, 'learning_rate': 0.0002, 'batch_size': 16}|\n",
        "| LSTM with selected features 1, FFT for Halpha    | 0.7801   | 0.3520    | 0.7683  | 0.4828   | {'hidden_size': 80, 'num_layers': 3, 'learning_rate': 0.0019, 'batch_size': 8} |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "The results demonstrate that incorporating specific features and tuning hyperparameters significantly impacts the performance of the LSTM model in predicting back-transitions in tokamak plasma. Among the tested configurations, using selected features 0 without FFT preprocessing achieved the best accuracy (0.8532) and F1 score (0.5699) indicating that feature selection combined with optimized hyperparameters leads to improved model performance. \n",
        "\n",
        "In contrast, configurations using FFT preprocessing generally enhanced recall but resulted in lower precision and F1 scores."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
